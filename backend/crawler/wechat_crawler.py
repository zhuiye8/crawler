"""å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çˆ¬è™«

âš ï¸ é‡è¦æ³•å¾‹å£°æ˜:
æ­¤æ¨¡å—ä»…ä¾›ä¸ªäººå­¦ä¹ ç ”ç©¶ä½¿ç”¨ã€‚
å•†ä¸šä½¿ç”¨å¯èƒ½è¿å:
1. å¾®ä¿¡å…¬ä¼—å¹³å°æœåŠ¡åè®®
2. ä¸­åäººæ°‘å…±å’Œå›½åä¸æ­£å½“ç«äº‰æ³•
3. å…¶ä»–ç›¸å…³æ³•å¾‹æ³•è§„

ä½¿ç”¨å‰è¯·ç¡®ä¿:
- å·²è·å¾—å†…å®¹æ‰€æœ‰è€…çš„æ˜ç¡®æˆæƒ
- ä»…ç”¨äºä¸ªäººå­¦ä¹ ç ”ç©¶ç›®çš„
- ä¸è¿›è¡Œå•†ä¸šåŒ–åˆ©ç”¨
- éµå®ˆçˆ¬å–é¢‘ç‡é™åˆ¶

å¼€å‘è€…ä¸æ‰¿æ‹…ä»»ä½•ç”±æ­¤äº§ç”Ÿçš„æ³•å¾‹è´£ä»»ã€‚
"""

import asyncio
import sys
from datetime import datetime
from patchright.async_api import async_playwright, TimeoutError
from bs4 import BeautifulSoup
from typing import List, Dict, Optional, Tuple
import re
import hashlib
import os
import json

# æ·»åŠ backendç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
from app.utils import timezone as tz


class WechatArticleCrawler:
    """
    å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çˆ¬è™«

    åŠŸèƒ½:
    - çˆ¬å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å®Œæ•´å†…å®¹
    - è§£ææ–‡ç« å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€å‘å¸ƒæ—¶é—´ç­‰ï¼‰
    - å¤„ç†æ–‡ç« ä¸­çš„å›¾ç‰‡
    - å®ç°åçˆ¬ç­–ç•¥ï¼ˆå»¶è¿Ÿã€é‡è¯•ç­‰ï¼‰
    - æ”¯æŒç¼“å­˜æœºåˆ¶
    """

    def __init__(self, headless: bool = True, rate_limit_delay: int = 10):
        """åˆå§‹åŒ–å¾®ä¿¡çˆ¬è™«"""
        self.headless = headless
        self.rate_limit_delay = rate_limit_delay

        # æµè§ˆå™¨çŠ¶æ€æŒä¹…åŒ–æ–‡ä»¶ï¼ˆä¿å­˜éªŒè¯åçš„Cookieï¼‰
        from pathlib import Path
        self.state_file = Path(__file__).parent.parent / 'cache' / 'wechat_browser_state.json'

        # User-Agentåˆ—è¡¨
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
        ]

    async def crawl_article(self, url: str, use_cache: bool = True) -> Optional[Dict]:
        """
        çˆ¬å–å•ç¯‡å¾®ä¿¡å…¬ä¼—å·æ–‡ç« 

        Args:
            url: å¾®ä¿¡æ–‡ç« URL (https://mp.weixin.qq.com/s/...)
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ (é»˜è®¤True)

        Returns:
            æ–‡ç« æ•°æ®å­—å…¸ï¼ŒåŒ…å«:
            - title: æ ‡é¢˜
            - author: ä½œè€…/å…¬ä¼—å·åç§°
            - published_at: å‘å¸ƒæ—¶é—´
            - summary: æ‘˜è¦
            - content_html: å®Œæ•´HTMLå†…å®¹
            - content_text: å®Œæ•´çº¯æ–‡æœ¬
            - tags: æ ‡ç­¾åˆ—è¡¨
            - url: åŸæ–‡é“¾æ¥

        Returns None if:
            - URLæ— æ•ˆ
            - çˆ¬å–å¤±è´¥
            - è§£æé”™è¯¯
        """
        # éªŒè¯URLæ ¼å¼
        if not self._is_valid_wechat_url(url):
            print(f"âš ï¸  æ— æ•ˆçš„å¾®ä¿¡æ–‡ç« URL: {url}")
            return None

        # æ£€æŸ¥ç¼“å­˜
        if use_cache:
            cached = self._load_from_cache(url)
            if cached:
                print(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½: {cached.get('title', 'Unknown')}")
                return cached

        # æ‰§è¡Œçˆ¬å–
        import random
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=self.headless)

            # å°è¯•åŠ è½½å·²ä¿å­˜çš„æµè§ˆå™¨çŠ¶æ€ï¼ˆå¤ç”¨éªŒè¯åçš„Cookieï¼‰
            if self.state_file.exists():
                print("ğŸ“¦ åŠ è½½å·²ä¿å­˜çš„æµè§ˆå™¨çŠ¶æ€...")
                context = await browser.new_context(storage_state=str(self.state_file))
            else:
                context = await browser.new_context()

            page = await context.new_page()

            try:
                print(f"ğŸ“„ æ­£åœ¨çˆ¬å–: {url[:50]}...")

                # è®¾ç½®éšæœºUser-Agent
                await page.set_extra_http_headers({
                    'User-Agent': random.choice(self.user_agents),
                })

                # è®¿é—®é¡µé¢
                await page.goto(url, wait_until="domcontentloaded", timeout=60000)

                # æ¨¡æ‹Ÿäººç±»é˜…è¯»è¡Œä¸ºï¼ˆé™ä½æ£€æµ‹é£é™©ï¼‰
                await self._simulate_human_behavior(page)

                # ç­‰å¾…æ–‡ç« å†…å®¹åŒºåŸŸåŠ è½½
                await page.wait_for_selector('#js_content', timeout=20000)

                # è·å–å¹¶è§£æHTML
                html = await page.content()
                soup = BeautifulSoup(html, 'lxml')

                # è§£ææ–‡ç« ä¿¡æ¯
                article = self._parse_article_page(soup, url)

                if article:
                    print(f"âœ… æˆåŠŸçˆ¬å–: {article['title']}")

                    # ä¿å­˜æµè§ˆå™¨çŠ¶æ€ï¼ˆä¸‹æ¬¡å¤ç”¨ï¼‰
                    await context.storage_state(path=str(self.state_file))

                    # ä¿å­˜åˆ°ç¼“å­˜
                    if use_cache:
                        self._save_to_cache(url, article)

                    # éšæœºå»¶è¿Ÿï¼ˆé¿å…è§„å¾‹æ€§æ£€æµ‹ï¼‰
                    delay = random.uniform(12, 18)
                    print(f"â±ï¸  ç­‰å¾… {delay:.1f} ç§’...")
                    await asyncio.sleep(delay)

                return article

            except TimeoutError:
                print(f"âŒ é¡µé¢åŠ è½½è¶…æ—¶: {url}")
                return None
            except Exception as e:
                print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
                return None
            finally:
                await context.close()
                await browser.close()

    def _is_valid_wechat_url(self, url: str) -> bool:
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å¾®ä¿¡å…¬ä¼—å·æ–‡ç« URL"""
        if not url:
            return False
        return url.startswith('https://mp.weixin.qq.com/s')

    async def _simulate_human_behavior(self, page):
        """æ¨¡æ‹Ÿäººç±»é˜…è¯»è¡Œä¸º"""
        import random

        # éšæœºæ»šåŠ¨2-4æ¬¡
        for _ in range(random.randint(2, 4)):
            scroll_px = random.randint(200, 500)
            await page.evaluate(f'window.scrollBy(0, {scroll_px})')
            await asyncio.sleep(random.uniform(0.8, 2.0))

        # åœç•™3-5ç§’
        await asyncio.sleep(random.uniform(3, 5))

    def _parse_article_page(self, soup: BeautifulSoup, url: str) -> Optional[Dict]:
        """
        è§£æå¾®ä¿¡æ–‡ç« é¡µé¢HTML

        Args:
            soup: BeautifulSoupå¯¹è±¡
            url: æ–‡ç« URL

        Returns:
            æ–‡ç« æ•°æ®å­—å…¸æˆ–None
        """
        try:
            # æå–æ ‡é¢˜
            title = self._extract_title(soup)
            if not title:
                print("âš ï¸  æœªèƒ½æå–æ–‡ç« æ ‡é¢˜")
                return None

            # æå–ä½œè€…/å…¬ä¼—å·åç§°
            author = self._extract_author(soup)

            # æå–å‘å¸ƒæ—¶é—´
            published_at = self._extract_publish_time(soup)

            # æå–æ‘˜è¦
            summary = self._extract_summary(soup)

            # æå–æ–‡ç« ä¸»ä½“å†…å®¹
            content_elem = soup.select_one('#js_content')
            if not content_elem:
                print("âš ï¸  æœªæ‰¾åˆ°æ–‡ç« å†…å®¹åŒºåŸŸ (#js_content)")
                return None

            # å¤„ç†å›¾ç‰‡URLï¼ˆå¾®ä¿¡å›¾ç‰‡ä½¿ç”¨data-srcï¼‰
            self._process_images(content_elem)

            # è·å–HTMLå’Œçº¯æ–‡æœ¬å†…å®¹
            content_html = str(content_elem)
            content_text = content_elem.get_text(separator="\n").strip()

            # æå–æ ‡ç­¾/å…³é”®è¯
            tags = self._extract_tags(soup)

            return {
                "title": title,
                "url": url,
                "author": author,
                "published_at": published_at,
                "summary": summary,
                "content_html": content_html,
                "content_text": content_text,
                "tags": tags,
                "source": "å¾®ä¿¡å…¬ä¼—å·",
                "crawled_at": tz.now()
            }

        except Exception as e:
            print(f"âš ï¸  è§£ææ–‡ç« é¡µé¢å¤±è´¥: {e}")
            return None

    def _extract_title(self, soup: BeautifulSoup) -> str:
        """æå–æ–‡ç« æ ‡é¢˜"""
        # æ–¹æ³•1: æŸ¥æ‰¾ #activity-name
        title_elem = soup.select_one('#activity-name')
        if title_elem:
            return title_elem.get_text().strip()

        # æ–¹æ³•2: æŸ¥æ‰¾ h1 æ ‡ç­¾
        h1_elem = soup.select_one('h1')
        if h1_elem:
            return h1_elem.get_text().strip()

        # æ–¹æ³•3: ä»metaæ ‡ç­¾æå–
        meta_title = soup.select_one('meta[property="og:title"]')
        if meta_title and meta_title.get('content'):
            return meta_title.get('content').strip()

        return ""

    def _extract_author(self, soup: BeautifulSoup) -> str:
        """æå–ä½œè€…/å…¬ä¼—å·åç§°"""
        # æŸ¥æ‰¾å…¬ä¼—å·åç§°
        author_elem = soup.select_one('#js_name')
        if author_elem:
            return author_elem.get_text().strip()

        # å¤‡ç”¨ï¼šä»metaæ ‡ç­¾æå–
        meta_author = soup.select_one('meta[name="author"]')
        if meta_author and meta_author.get('content'):
            return meta_author.get('content').strip()

        return "æœªçŸ¥ä½œè€…"

    def _extract_publish_time(self, soup: BeautifulSoup) -> datetime:
        """æå–å‘å¸ƒæ—¶é—´"""
        # æŸ¥æ‰¾å‘å¸ƒæ—¶é—´å…ƒç´ 
        time_elem = soup.select_one('#publish_time')
        if time_elem:
            time_str = time_elem.get_text().strip()
            return self._parse_time_string(time_str)

        # å¤‡ç”¨ï¼šæŸ¥æ‰¾metaæ ‡ç­¾
        meta_time = soup.select_one('meta[property="og:article:published_time"]')
        if meta_time and meta_time.get('content'):
            time_str = meta_time.get('content')
            return self._parse_time_string(time_str)

        # è§£æå¤±è´¥è¿”å›å½“å‰æ—¶é—´
        return tz.now()

    def _parse_time_string(self, time_str: str) -> datetime:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡"""
        try:
            # æ¸…ç†æ—¶é—´å­—ç¬¦ä¸²
            time_str = time_str.strip()

            # å°è¯•ä¸åŒçš„æ—¶é—´æ ¼å¼
            formats = [
                "%Y-%m-%d",
                "%Y/%m/%d",
                "%Yå¹´%mæœˆ%dæ—¥",
                "%Y-%m-%d %H:%M",
                "%Y-%m-%d %H:%M:%S",
                "%Y/%m/%d %H:%M",
            ]

            for fmt in formats:
                try:
                    return datetime.strptime(time_str, fmt)
                except ValueError:
                    continue

            # ä½¿ç”¨æ­£åˆ™æå–æ—¥æœŸ
            match = re.search(r'(\d{4})[-/å¹´](\d{1,2})[-/æœˆ](\d{1,2})', time_str)
            if match:
                year, month, day = match.groups()
                return datetime(int(year), int(month), int(day))

        except Exception as e:
            print(f"âš ï¸  æ—¶é—´è§£æå¤±è´¥: {e}")

        # è§£æå¤±è´¥è¿”å›å½“å‰æ—¶é—´
        return tz.now()

    def _extract_summary(self, soup: BeautifulSoup) -> str:
        """æå–æ–‡ç« æ‘˜è¦"""
        # ä»meta descriptionæå–
        desc_elem = soup.select_one('meta[name="description"]')
        if desc_elem and desc_elem.get('content'):
            return desc_elem.get('content').strip()

        # ä»og:descriptionæå–
        og_desc = soup.select_one('meta[property="og:description"]')
        if og_desc and og_desc.get('content'):
            return og_desc.get('content').strip()

        return ""

    def _process_images(self, content_elem: BeautifulSoup):
        """
        å¤„ç†æ–‡ç« ä¸­çš„å›¾ç‰‡
        å¾®ä¿¡å›¾ç‰‡é€šå¸¸ä½¿ç”¨data-srcå±æ€§å­˜å‚¨çœŸå®URL
        """
        images = content_elem.select('img')
        for img in images:
            # å¾®ä¿¡å›¾ç‰‡çš„çœŸå®URLåœ¨data-srcä¸­
            data_src = img.get('data-src', '')
            if data_src:
                img['src'] = data_src

            # å¤„ç†å…¶ä»–å¯èƒ½çš„å±æ€§
            data_url = img.get('data-url', '')
            if data_url and not data_src:
                img['src'] = data_url

    def _extract_tags(self, soup: BeautifulSoup) -> List[str]:
        """æå–æ–‡ç« æ ‡ç­¾/å…³é”®è¯"""
        tags = []

        # ä»meta keywordsæå–
        keywords_elem = soup.select_one('meta[name="keywords"]')
        if keywords_elem and keywords_elem.get('content'):
            keywords = keywords_elem.get('content', '')
            tags = [k.strip() for k in keywords.split(',') if k.strip()]

        return tags

    async def crawl_with_retry(self, url: str, max_retries: int = 3) -> Optional[Dict]:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„çˆ¬å–

        Args:
            url: æ–‡ç« URL
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            æ–‡ç« æ•°æ®æˆ–None
        """
        for attempt in range(max_retries):
            try:
                article = await self.crawl_article(url, use_cache=True)
                if article:
                    return article

                print(f"âš ï¸  ç¬¬ {attempt + 1}/{max_retries} æ¬¡å°è¯•å¤±è´¥ï¼Œç­‰å¾…é‡è¯•...")

                # é€’å¢å»¶è¿Ÿï¼ˆé¿å…å¿«é€Ÿé‡è¯•è¢«å°ï¼‰
                retry_delay = 5 * (attempt + 1)
                await asyncio.sleep(retry_delay)

            except Exception as e:
                print(f"âŒ çˆ¬å–å‡ºé”™ (å°è¯• {attempt + 1}/{max_retries}): {e}")

        # æ‰€æœ‰é‡è¯•å¤±è´¥
        print(f"âŒ çˆ¬å–å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
        return None

    def _get_cache_path(self, url: str) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        # åˆ›å»ºç¼“å­˜ç›®å½•
        cache_dir = Path(__file__).parent.parent / 'cache' / 'wechat_articles'
        cache_dir.mkdir(parents=True, exist_ok=True)

        # ä½¿ç”¨URLçš„MD5ä½œä¸ºæ–‡ä»¶å
        url_hash = hashlib.md5(url.encode()).hexdigest()
        return str(cache_dir / f"{url_hash}.json")

    def _save_to_cache(self, url: str, article: Dict):
        """ä¿å­˜æ–‡ç« åˆ°ç¼“å­˜"""
        try:
            cache_path = self._get_cache_path(url)
            with open(cache_path, 'w', encoding='utf-8') as f:
                # è½¬æ¢datetimeä¸ºå­—ç¬¦ä¸²ä»¥ä¾¿JSONåºåˆ—åŒ–
                article_copy = article.copy()
                for key in ['published_at', 'crawled_at']:
                    if key in article_copy and isinstance(article_copy[key], datetime):
                        article_copy[key] = article_copy[key].isoformat()

                json.dump(article_copy, f, ensure_ascii=False, indent=2)

            print(f"ğŸ’¾ å·²ç¼“å­˜åˆ°: {cache_path}")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def _load_from_cache(self, url: str) -> Optional[Dict]:
        """ä»ç¼“å­˜åŠ è½½æ–‡ç« """
        try:
            cache_path = self._get_cache_path(url)
            if not os.path.exists(cache_path):
                return None

            with open(cache_path, 'r', encoding='utf-8') as f:
                article = json.load(f)

            # è½¬æ¢æ—¶é—´å­—ç¬¦ä¸²å›datetimeå¯¹è±¡
            for key in ['published_at', 'crawled_at']:
                if key in article and isinstance(article[key], str):
                    article[key] = datetime.fromisoformat(article[key])

            return article
        except Exception as e:
            print(f"âš ï¸  è¯»å–ç¼“å­˜å¤±è´¥: {e}")
            return None


# ä¸ºäº†é¿å…æœªå¯¼å…¥Pathé”™è¯¯
from pathlib import Path
