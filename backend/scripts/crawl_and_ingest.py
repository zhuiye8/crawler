"""ä»è¯æ¸¡äº‘çˆ¬å–æ–‡ç« å¹¶å­˜å…¥æ•°æ®åº“ï¼ˆé›†æˆå¾®ä¿¡å…¬ä¼—å·çˆ¬å–ï¼‰"""

import asyncio
import sys
import hashlib
import argparse
from pathlib import Path
from datetime import datetime, timedelta

sys.path.append(str(Path(__file__).parent.parent))

from sqlalchemy import select
from app.database import get_db_context
from app.models import Article, Source
from app.utils.html_cleaner import clean_html
from app.utils.s3_client import get_s3_client
from app.utils import timezone as tz
from app.config import settings
from crawler.pharnex_crawler import PharnexCrawler
from crawler.wechat_crawler import WechatArticleCrawler

# é…ç½®é€‰é¡¹
ENABLE_WECHAT_CRAWL = True  # æ˜¯å¦å¯ç”¨å¾®ä¿¡å…¬ä¼—å·çˆ¬å–
WECHAT_CRAWL_DELAY = 10  # å¾®ä¿¡çˆ¬å–é—´éš”ï¼ˆç§’ï¼‰


async def ingest_articles(articles: list):
    """å°†çˆ¬å–çš„æ–‡ç« å­˜å…¥æ•°æ®åº“ï¼ˆåŒ…å«å¾®ä¿¡å…¬ä¼—å·å†…å®¹ï¼‰"""
    s3_client = get_s3_client()
    pharnex_crawler = PharnexCrawler()
    wechat_crawler = WechatArticleCrawler(
        headless=True,
        rate_limit_delay=WECHAT_CRAWL_DELAY
    ) if ENABLE_WECHAT_CRAWL else None

    async with get_db_context() as db:
        # è·å–æ•°æ®æº
        result = await db.execute(select(Source).where(Source.name == "è¯æ¸¡äº‘"))
        source = result.scalar_one_or_none()
        if not source:
            print("âŒ æœªæ‰¾åˆ°æ•°æ®æº 'è¯æ¸¡äº‘'ï¼Œè¯·å…ˆè¿è¡Œ init_db.py")
            return

        for article_data in articles:
            try:
                print(f"\nğŸ“ å¤„ç†æ–‡ç« : {article_data['title']}")

                # çˆ¬å–è¯æ¸¡äº‘è¯¦æƒ…é¡µï¼ˆåŒ…å«å¾®ä¿¡åŸæ–‡é“¾æ¥æå–ï¼‰
                content_html, content_text, original_source_url = await pharnex_crawler.crawl_detail_page(
                    article_data["url"]
                )

                if not content_text:
                    print(f"âš ï¸  æœªæ‰¾åˆ°å†…å®¹ï¼Œè·³è¿‡")
                    continue

                # æ¸…ç†å†…å®¹
                content_text = clean_html(content_html) if content_html else content_text

                # åˆå§‹åŒ–å¾®ä¿¡å†…å®¹å­—æ®µ
                wechat_content_html = None
                wechat_content_text = None
                content_source = "pharnexcloud"

                # å¦‚æœæ‰¾åˆ°å¾®ä¿¡åŸæ–‡é“¾æ¥ä¸”å¯ç”¨äº†å¾®ä¿¡çˆ¬å–
                if original_source_url and wechat_crawler:
                    print(f"  ğŸ”— å‘ç°å¾®ä¿¡åŸæ–‡é“¾æ¥ï¼Œå°è¯•çˆ¬å–...")
                    try:
                        wechat_article = await wechat_crawler.crawl_with_retry(
                            original_source_url,
                            max_retries=2  # æœ€å¤šé‡è¯•2æ¬¡
                        )

                        if wechat_article:
                            wechat_content_html = wechat_article.get('content_html')
                            wechat_content_text = wechat_article.get('content_text')
                            content_source = "wechat"
                            print(f"  âœ… æˆåŠŸçˆ¬å–å¾®ä¿¡å†…å®¹ ({len(wechat_content_text)} å­—ç¬¦)")

                            # ä½¿ç”¨å¾®ä¿¡å†…å®¹ä½œä¸ºä¸»å†…å®¹ï¼ˆå¦‚æœæ›´å®Œæ•´ï¼‰
                            if wechat_content_text and len(wechat_content_text) > len(content_text):
                                content_text = wechat_content_text
                                content_html = wechat_content_html
                                print(f"  â„¹ï¸  ä½¿ç”¨å¾®ä¿¡å†…å®¹ä½œä¸ºä¸»å†…å®¹")
                        else:
                            print(f"  âš ï¸  å¾®ä¿¡çˆ¬å–å¤±è´¥ï¼Œé™çº§ä½¿ç”¨è¯æ¸¡äº‘å†…å®¹")

                    except Exception as e:
                        print(f"  âš ï¸  å¾®ä¿¡çˆ¬å–å‡ºé”™: {e}ï¼Œé™çº§ä½¿ç”¨è¯æ¸¡äº‘å†…å®¹")

                # è®¡ç®—å»é‡å“ˆå¸Œ
                canonical_hash = hashlib.sha256(content_text.encode()).hexdigest()

                # æ£€æŸ¥æœªåˆ é™¤çš„æ–‡ç« æ˜¯å¦å·²å­˜åœ¨
                active_result = await db.execute(
                    select(Article).where(
                        Article.canonical_hash == canonical_hash,
                        Article.is_deleted == False
                    )
                )
                active_article = active_result.scalar_one_or_none()

                if active_article:
                    print(f"  â­ï¸  æ–‡ç« å·²å­˜åœ¨: {active_article.title}")
                    continue

                # æ£€æŸ¥å·²åˆ é™¤çš„æ–‡ç« 
                deleted_result = await db.execute(
                    select(Article).where(
                        Article.canonical_hash == canonical_hash,
                        Article.is_deleted == True
                    )
                )
                deleted_article = deleted_result.scalar_one_or_none()

                # ç”Ÿæˆæ–‡ç« IDç”¨äºS3å­˜å‚¨
                article_id = f"article_{int(tz.now().timestamp() * 1000)}"

                # ä¸Šä¼ åˆ°S3
                s3_key_raw = f"{article_id}/original.html"
                s3_key_clean = f"{article_id}/cleaned.txt"

                s3_client.upload_text(content_html or "", settings.S3_BUCKET_RAW, s3_key_raw)
                s3_client.upload_text(content_text, settings.S3_BUCKET_CLEAN, s3_key_clean)

                # å¦‚æœæœ‰å¾®ä¿¡å†…å®¹ï¼Œä¹Ÿä¿å­˜å¾®ä¿¡ç‰ˆæœ¬
                if wechat_content_html:
                    s3_key_wechat = f"{article_id}/wechat_original.html"
                    s3_client.upload_text(wechat_content_html, settings.S3_BUCKET_RAW, s3_key_wechat)

                if deleted_article:
                    # æ›´æ–°å·²åˆ é™¤æ–‡ç« çš„æ•°æ®å¹¶æ¢å¤
                    deleted_article.title = article_data["title"]
                    deleted_article.summary = article_data.get("summary", "")
                    deleted_article.author = article_data.get("author", "è¯æ¸¡äº‘")
                    deleted_article.category = article_data.get("category", "å‰æ²¿ç ”ç©¶")
                    deleted_article.tags = article_data.get("tags", [])
                    deleted_article.published_at = article_data["published_at"]
                    deleted_article.content_url = article_data["url"]
                    deleted_article.content_text = content_text
                    deleted_article.content_source = content_source
                    deleted_article.original_source_url = original_source_url or None
                    deleted_article.wechat_content_html = wechat_content_html
                    deleted_article.wechat_content_text = wechat_content_text
                    deleted_article.crawled_at = tz.now()
                    deleted_article.is_deleted = False  # æ¢å¤æ–‡ç« 

                    await db.commit()
                    await db.refresh(deleted_article)

                    print(f"  â™»ï¸  æ¢å¤å¹¶æ›´æ–°å·²åˆ é™¤æ–‡ç« : {deleted_article.title} (ID: {deleted_article.id})")
                else:
                    # åˆ›å»ºæ–°æ–‡ç« è®°å½•
                    article = Article(
                        title=article_data["title"],
                        summary=article_data.get("summary", ""),
                        author=article_data.get("author", "è¯æ¸¡äº‘"),
                        source_id=source.id,
                        category=article_data.get("category", "å‰æ²¿ç ”ç©¶"),
                        tags=article_data.get("tags", []),
                        published_at=article_data["published_at"],
                        content_url=article_data["url"],
                        content_text=content_text,
                        content_source=content_source,
                        original_source_url=original_source_url or None,
                        wechat_content_html=wechat_content_html,
                        wechat_content_text=wechat_content_text,
                        canonical_hash=canonical_hash,
                    )

                    db.add(article)
                    await db.commit()
                    await db.refresh(article)

                    print(f"  âœ… å·²å…¥åº“: {article.title} (ID: {article.id}, æ¥æº: {content_source})")
                await asyncio.sleep(1)  # è¯·æ±‚é™æµ

            except Exception as e:
                print(f"  âŒ å¤„ç†æ–‡ç« å‡ºé”™: {e}")
                await db.rollback()
                continue


async def main(args):
    """ä¸»å‡½æ•°ï¼šçˆ¬å–è¯æ¸¡äº‘æ–‡ç« å¹¶å­˜å…¥æ•°æ®åº“"""
    print("ğŸš€ å¼€å§‹çˆ¬å–...")
    print(f"ğŸ“Œ å¾®ä¿¡çˆ¬å–: {'âœ… å¯ç”¨' if ENABLE_WECHAT_CRAWL else 'âŒ ç¦ç”¨'}")

    # è®¡ç®—æ—¶é—´èŒƒå›´ï¼ˆç»Ÿä¸€å½’é›¶åˆ°0ç‚¹ï¼Œé¿å…æ—¶åˆ†ç§’å¹²æ‰°ï¼‰
    from_date = None
    to_date = None

    if args.days_back:
        # è·å–ä»Šå¤©0ç‚¹
        from_date = tz.today_start() - timedelta(days=args.days_back)
        print(f"ğŸ“… æ—¶é—´èŒƒå›´: æœ€è¿‘ {args.days_back} å¤© (ä» {from_date.strftime('%Y-%m-%d')} å¼€å§‹)")
    elif args.from_date:
        from_date = tz.naive_to_china(datetime.strptime(args.from_date, "%Y-%m-%d"))
        if args.to_date:
            to_date = tz.naive_to_china(datetime.strptime(args.to_date, "%Y-%m-%d"))
            # ç»“æŸæ—¥æœŸè®¾ä¸ºå½“å¤©23:59:59
            to_date = to_date.replace(hour=23, minute=59, second=59)
            print(f"ğŸ“… æ—¶é—´èŒƒå›´: {args.from_date} è‡³ {args.to_date}")
        else:
            print(f"ğŸ“… æ—¶é—´èŒƒå›´: {args.from_date} è‡³ä»Š")

    # æ‰“å°çˆ¬å–é…ç½®
    print(f"ğŸ“„ é¡µæ•°é™åˆ¶: {args.pages} é¡µ")
    if args.max_articles:
        print(f"ğŸ“Š æ–‡ç« æ•°é™åˆ¶: {args.max_articles} ç¯‡")

    # çˆ¬å–æ–‡ç« 
    crawler = PharnexCrawler(headless=True)
    articles = await crawler.crawl_multiple_pages(
        num_pages=args.pages,
        max_articles=args.max_articles,
        from_date=from_date,
        to_date=to_date
    )

    print(f"\nğŸ“Š å…±çˆ¬å– {len(articles)} ç¯‡æ–‡ç« ")
    print("ğŸ’¾ å¼€å§‹å­˜å…¥æ•°æ®åº“...")

    await ingest_articles(articles)

    print("\nğŸ‰ çˆ¬å–å’Œå…¥åº“å®Œæˆï¼")
    print("\nğŸ’¡ æç¤º:")
    print("  - å¦‚éœ€ç¦ç”¨å¾®ä¿¡çˆ¬å–ï¼Œä¿®æ”¹ ENABLE_WECHAT_CRAWL = False")
    print("  - å¦‚éœ€è°ƒæ•´å¾®ä¿¡çˆ¬å–é—´éš”ï¼Œä¿®æ”¹ WECHAT_CRAWL_DELAY")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="è¯æ¸¡äº‘æ–‡ç« çˆ¬è™«")

    # æ•°é‡æ§åˆ¶
    parser.add_argument("--pages", type=int, default=10, help="çˆ¬å–é¡µæ•° (é»˜è®¤: 10)")
    parser.add_argument("--max-articles", type=int, default=None, help="æœ€å¤§æ–‡ç« æ•° (å¯é€‰)")

    # æ—¶é—´èŒƒå›´
    parser.add_argument("--days-back", type=int, default=None, help="æœ€è¿‘Nå¤© (ä¾‹å¦‚: 7, 30)")
    parser.add_argument("--from-date", type=str, default=None, help="èµ·å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)")
    parser.add_argument("--to-date", type=str, default=None, help="ç»“æŸæ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)")

    args = parser.parse_args()

    asyncio.run(main(args))
